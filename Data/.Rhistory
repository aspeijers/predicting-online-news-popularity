H.mle.new <- PHI.new%*%solve(t(PHI.new)%*%PHI.new)%*%t(PHI.new)
e.new <- (diag(n)-H.mle.new) %*% y
R.sq.new <- as.numeric( 1 - (t(e.new) %*% e.new) / sum((y - mean(y))^2) )
marg.lik.new <- as.numeric(factorial((n-1)/2 - 1) / (pi^((n-1)/2) * sqrt(n)) * (sqrt(t(e) %*% e))^(1-n) * (1+g)^((n-d.new-2)/2) / (1+g*(1-R.sq.new))^((n-1)/2) )
#Save w estimates (w computed based on the w_bayes estimate expression for Zelner's g prior)
w.history[i,which(gamma!=0)] = ((g/ (g+1)) * solve(t(PHI.new)%*% PHI.new)%*%t(PHI.new)%*%y)[-1]
#Save probabilities
prob.new <- marg.lik.new * model.prior[d.new+1]
prob.history[i] <- marg.lik.new
#Save gamma
gamma.history[i,] <- gamma
#Set gamma to gamma new
#gamma <- gamma.new
}
no.vars <- rowSums(gamma.history)
plot(1:L, no.vars, type="l",xlab="Iteration", ylab="Number of variables in the model", main="Model size")
abline(mean(no.vars), 0, col="red")
cummean <- matrix(NA, L, m)
for (i in 1:L) {
cummean[i,] <- colSums(gamma.history[1:i,,drop=F])/i
}
matplot(1:L, cummean, type="l", lty=1, xlab="Step", ylab="marginal inclusion probability", main="Marginal Inclusion Probabilities for each of the 50 Features", las=1)
plot(cummean[1000,1:40], xlim=c(0,50), ylim=c(0,1), xlab="Predictor", ylab="Posterior probability of non-zero coefficient", main="Marginal Inclusion Probabilities")
points(41:45, cummean[1000, 41:45], col="blue")
points(46:50, cummean[1000, 46:50], col="red")
abline(1, 0, col="red")
abline(0.5, 0, col="blue")
w.history[is.na(w.history)] <- 0
plot(colMeans(w.history, na.rm = TRUE)[1:40], xlim=c(0,50), ylim=c(min(w.history),max(w.history)), xlab="Predictor", ylab="Expected posterior coefficients", main="Bayesian Model Averaging - Estimates of Coefficients")
points(41:45, colMeans(w.history, na.rm = TRUE)[41:45], col="blue")
points(46:50, colMeans(w.history, na.rm = TRUE)[46:50], col="red")
abline(1, 0, col="red")
abline(0.5, 0, col="blue")
w.history[is.na(w.history)] <- 0
plot(colMeans(w.history, na.rm = TRUE)[1:40], xlim=c(0,50), ylim=c(-0.5, 1.5, xlab="Predictor", ylab="Expected posterior coefficients", main="Bayesian Model Averaging - Estimates of Coefficients")
points(41:45, colMeans(w.history, na.rm = TRUE)[41:45], col="blue")
points(46:50, colMeans(w.history, na.rm = TRUE)[46:50], col="red")
abline(1, 0, col="red")
abline(0.5, 0, col="blue")
w.history[is.na(w.history)] <- 0
plot(colMeans(w.history, na.rm = TRUE)[1:40], xlim=c(0,50), ylim=c(-0.5, 1.5), xlab="Predictor", ylab="Expected posterior coefficients", main="Bayesian Model Averaging - Estimates of Coefficients")
points(41:45, colMeans(w.history, na.rm = TRUE)[41:45], col="blue")
points(46:50, colMeans(w.history, na.rm = TRUE)[46:50], col="red")
abline(1, 0, col="red")
abline(0.5, 0, col="blue")
sum(cummean>0.8)
sum(cummean[1000,]>0.8)
sum(cummean[1000,]>0.6)
library(mvtnorm)
library(mombf)
# true coefficients of variables 1-50:
# First 40 variables have no influence, next 5 have a bit and last 5 have a lot.
w.true <- c(rep(0,40), rep(.5,5), rep(1,5))
m <- length(w.true)
# create covariance matrix - highly correlated variables.
sigma <- diag(m)
sigma[upper.tri(sigma)] <- 0.75
sigma[lower.tri(sigma)] <- 0.75
# simulate data using above covariance and mean zero - n observations
n <- 100
set.seed(12345)
PHI <- rmvnorm(n, sigma=sigma)
PHI <- cbind(1, PHI) #add in intercept
# simulate outomes - standard normal error added to each one
y <- PHI[,-1] %*% matrix(w.true, ncol=1) + rnorm(n)
e = y - mean(y)
# Run Bayesian model selection
#fit1 <- modelSelection(y=y, x=PHI, niter=10^4, priorCoef = zellnerprior(tau=n), priorDelta = modelbbprior(), burnin=0)
### Gibbs Sampling
# number of iterations
L <- 10000
# Prior on model space using Beta-Binomial. Compute and save it outside the loop for every possible model size.
d.seq <- seq(0, m, 1)
model.prior <- rep(0, m+1)
for (i in 1:(m+1)) {
model.prior[i] <- 1/(m+1) * 1/( factorial(m) / (factorial(m - d.seq[i]) * factorial(d.seq[i])) )
}
# initial gamma
gamma.0 <- rep(0,50)
gamma <- gamma.0
# prior dispersion - we could extend this to run for various values of g
g <- n
#setup matrices to store results in each iteration
gamma.history <- matrix(NA, L, m)
prob.history <- rep(NA,L)
w.history <- matrix(NA, L, m)
# update loop
set.seed(14234)
for (i in 1:L) {
#Initialize new gamma
#gamma.new <- gamma
for (j in 1:m) {
#Compute probabilities without var j
#number of parameters and gamma without j
d.0 <- sum(gamma[-j]!=0)
gamma.test.0 <- gamma
gamma.test.0[j] <- 0
#Compute MLE r-squared without j
PHI.0 <- PHI[,c(TRUE,(gamma.test.0==1))]
H.mle.0 <- PHI.0%*%solve(t(PHI.0)%*%PHI.0)%*%t(PHI.0)
e.0 <- (diag(n)-H.mle.0) %*% y
R.sq.0 <- as.numeric( 1 - (t(e.0) %*% e.0) / sum((y - mean(y))^2) )
#Compute second term of marginal likelihood (first term same for all models).
#Expression is Omiros notes was wrong. Corrected based on other notes.
marg.lik.0 <- as.numeric( (1+g)^((n-d.0-2)/2) / (1+g*(1-R.sq.0))^((n-1)/2))
#Compute model probability with var j
#number of parameters and gamma with j
d.1 <- d.0 + 1
gamma.test.1 <- gamma
gamma.test.1[j] <- 1
#Compute MLE r-squared with j
PHI.1 <- PHI[,c(TRUE,(gamma.test.1==1))]
H.mle.1 <- PHI.1%*%solve(t(PHI.1)%*%PHI.1)%*%t(PHI.1)
e.1 <- (diag(n)-H.mle.1) %*% y
R.sq.1 <- as.numeric( 1 - (t(e.1) %*% e.1) / sum((y - mean(y))^2) )
#Compute second term of marginal likelihood with j
marg.lik.1 <- as.numeric( (1+g)^((n-(d.1)-2)/2) / (1+g*(1-R.sq.1))^((n-1)/2) )
# compute probability of new variable being in model
prob <- marg.lik.1 * model.prior[d.1+1] / (marg.lik.1 * model.prior[d.1+1] + marg.lik.0 * model.prior[d.0+1])
# update gamma and variables
if (runif(1) < prob) {
gamma[j] <- 1
}
else {
gamma[j] <- 0
}
}
#Compute results for the new model
d.new <- sum(gamma!=0)
PHI.new <- PHI[,c(TRUE,(gamma==1))]
H.mle.new <- PHI.new%*%solve(t(PHI.new)%*%PHI.new)%*%t(PHI.new)
e.new <- (diag(n)-H.mle.new) %*% y
R.sq.new <- as.numeric( 1 - (t(e.new) %*% e.new) / sum((y - mean(y))^2) )
marg.lik.new <- as.numeric(factorial((n-1)/2 - 1) / (pi^((n-1)/2) * sqrt(n)) * (sqrt(t(e) %*% e))^(1-n) * (1+g)^((n-d.new-2)/2) / (1+g*(1-R.sq.new))^((n-1)/2) )
#Save w estimates (w computed based on the w_bayes estimate expression for Zelner's g prior)
w.history[i,which(gamma!=0)] = ((g/ (g+1)) * solve(t(PHI.new)%*% PHI.new)%*%t(PHI.new)%*%y)[-1]
#Save probabilities
prob.new <- marg.lik.new * model.prior[d.new+1]
prob.history[i] <- marg.lik.new
#Save gamma
gamma.history[i,] <- gamma
#Set gamma to gamma new
#gamma <- gamma.new
}
no.vars <- rowSums(gamma.history)
plot(1:L, no.vars, type="l",xlab="Iteration", ylab="Number of variables in the model", main="Model size")
abline(mean(no.vars), 0, col="red")
cummean <- matrix(NA, L, m)
for (i in 1:L) {
cummean[i,] <- colSums(gamma.history[1:i,,drop=F])/i
}
matplot(1:L, cummean, type="l", lty=1, xlab="Step", ylab="marginal inclusion probability", main="Marginal Inclusion Probabilities for each of the 50 Features", las=1)
plot(cummean[1000,1:40], xlim=c(0,50), ylim=c(0,1), xlab="Predictor", ylab="Posterior probability of non-zero coefficient", main="Marginal Inclusion Probabilities")
points(41:45, cummean[1000, 41:45], col="blue")
points(46:50, cummean[1000, 46:50], col="red")
abline(1, 0, col="red")
abline(0.5, 0, col="blue")
w.history[is.na(w.history)] <- 0
plot(colMeans(w.history, na.rm = TRUE)[1:40], xlim=c(0,50), ylim=c(-0.5, 1.5), xlab="Predictor", ylab="Expected posterior coefficients", main="Bayesian Model Averaging - Estimates of Coefficients")
points(41:45, colMeans(w.history, na.rm = TRUE)[41:45], col="blue")
points(46:50, colMeans(w.history, na.rm = TRUE)[46:50], col="red")
abline(1, 0, col="red")
abline(0.5, 0, col="blue")
mean(no.vars)
library(mvtnorm)
library(mombf)
# true coefficients of variables 1-50:
# First 40 variables have no influence, next 5 have a bit and last 5 have a lot.
w.true <- c(rep(0,40), rep(.5,5), rep(1,5))
m <- length(w.true)
# create covariance matrix - highly correlated variables.
sigma <- diag(m)
sigma[upper.tri(sigma)] <- 0.75
sigma[lower.tri(sigma)] <- 0.75
# simulate data using above covariance and mean zero - n observations
n <- 100
set.seed(12345)
PHI <- rmvnorm(n, sigma=sigma)
PHI <- cbind(1, PHI) #add in intercept
# simulate outomes - standard normal error added to each one
y <- PHI[,-1] %*% matrix(w.true, ncol=1) + rnorm(n)
e = y - mean(y)
# Run Bayesian model selection
#fit1 <- modelSelection(y=y, x=PHI, niter=10^4, priorCoef = zellnerprior(tau=n), priorDelta = modelbbprior(), burnin=0)
### Gibbs Sampling
# number of iterations
L <- 1000
d.seq <- seq(0, m, 1)
model.prior <- rep(0, m+1)
for (i in 1:(m+1)) {
model.prior[i] <- 1/(m+1) * 1/( factorial(m) / (factorial(m - d.seq[i]) * factorial(d.seq[i])) )
}
# initial gamma
gamma.0 <- rep(0,50)
gamma <- gamma.0
# prior dispersion - we could extend this to run for various values of g
g <- n
#setup matrices to store results in each iteration
gamma.history <- matrix(NA, L, m)
prob.history <- rep(NA,L)
w.history <- matrix(NA, L, m)
# update loop
set.seed(14234)
for (i in 1:L) {
#Initialize new gamma
#gamma.new <- gamma
for (j in 1:m) {
#Compute probabilities without var j
#number of parameters and gamma without j
d.0 <- sum(gamma[-j]!=0)
gamma.test.0 <- gamma
gamma.test.0[j] <- 0
#Compute MLE r-squared without j
PHI.0 <- PHI[,c(TRUE,(gamma.test.0==1))]
H.mle.0 <- PHI.0%*%solve(t(PHI.0)%*%PHI.0)%*%t(PHI.0)
e.0 <- (diag(n)-H.mle.0) %*% y
R.sq.0 <- as.numeric( 1 - (t(e.0) %*% e.0) / sum((y - mean(y))^2) )
#Compute second term of marginal likelihood (first term same for all models).
#Expression is Omiros notes was wrong. Corrected based on other notes.
marg.lik.0 <- as.numeric( (1+g)^((n-d.0-2)/2) / (1+g*(1-R.sq.0))^((n-1)/2))
#Compute model probability with var j
#number of parameters and gamma with j
d.1 <- d.0 + 1
gamma.test.1 <- gamma
gamma.test.1[j] <- 1
#Compute MLE r-squared with j
PHI.1 <- PHI[,c(TRUE,(gamma.test.1==1))]
H.mle.1 <- PHI.1%*%solve(t(PHI.1)%*%PHI.1)%*%t(PHI.1)
e.1 <- (diag(n)-H.mle.1) %*% y
R.sq.1 <- as.numeric( 1 - (t(e.1) %*% e.1) / sum((y - mean(y))^2) )
#Compute second term of marginal likelihood with j
marg.lik.1 <- as.numeric( (1+g)^((n-(d.1)-2)/2) / (1+g*(1-R.sq.1))^((n-1)/2) )
# compute probability of new variable being in model
prob <- marg.lik.1 * model.prior[d.1+1] / (marg.lik.1 * model.prior[d.1+1] + marg.lik.0 * model.prior[d.0+1])
# update gamma and variables
if (runif(1) < prob) {
gamma[j] <- 1
}
else {
gamma[j] <- 0
}
}
#Compute results for the new model
d.new <- sum(gamma!=0)
PHI.new <- PHI[,c(TRUE,(gamma==1))]
H.mle.new <- PHI.new%*%solve(t(PHI.new)%*%PHI.new)%*%t(PHI.new)
e.new <- (diag(n)-H.mle.new) %*% y
R.sq.new <- as.numeric( 1 - (t(e.new) %*% e.new) / sum((y - mean(y))^2) )
marg.lik.new <- as.numeric(factorial((n-1)/2 - 1) / (pi^((n-1)/2) * sqrt(n)) * (sqrt(t(e) %*% e))^(1-n) * (1+g)^((n-d.new-2)/2) / (1+g*(1-R.sq.new))^((n-1)/2) )
#Save w estimates (w computed based on the w_bayes estimate expression for Zelner's g prior)
w.history[i,which(gamma!=0)] = ((g/ (g+1)) * solve(t(PHI.new)%*% PHI.new)%*%t(PHI.new)%*%y)[-1]
#Save probabilities
prob.new <- marg.lik.new * model.prior[d.new+1]
prob.history[i] <- marg.lik.new
#Save gamma
gamma.history[i,] <- gamma
#Set gamma to gamma new
#gamma <- gamma.new
}
no.vars <- rowSums(gamma.history)
plot(1:L, no.vars, type="l",xlab="Iteration", ylab="Number of variables in the model", main="Model size")
abline(mean(no.vars), 0, col="red")
mean(no.vars)
cummean <- matrix(NA, L, m)
for (i in 1:L) {
cummean[i,] <- colSums(gamma.history[1:i,,drop=F])/i
}
matplot(1:L, cummean, type="l", lty=1, xlab="Step", ylab="marginal inclusion probability", main="Marginal Inclusion Probabilities for each of the 50 Features", las=1)
plot(cummean[1000,1:40], xlim=c(0,50), ylim=c(0,1), xlab="Predictor", ylab="Posterior probability of non-zero coefficient", main="Marginal Inclusion Probabilities")
points(41:45, cummean[1000, 41:45], col="blue")
points(46:50, cummean[1000, 46:50], col="red")
abline(1, 0, col="red")
abline(0.5, 0, col="blue")
w.history[is.na(w.history)] <- 0
plot(colMeans(w.history, na.rm = TRUE)[1:40], xlim=c(0,50), ylim=c(-0.5, 1.5), xlab="Predictor", ylab="Expected posterior coefficients", main="Bayesian Model Averaging - Estimates of Coefficients")
points(41:45, colMeans(w.history, na.rm = TRUE)[41:45], col="blue")
points(46:50, colMeans(w.history, na.rm = TRUE)[46:50], col="red")
abline(1, 0, col="red")
abline(0.5, 0, col="blue")
library(mvtnorm)
library(mombf)
# true coefficients of variables 1-50:
# First 40 variables have no influence, next 5 have a bit and last 5 have a lot.
w.true <- c(rep(0,40), rep(.5,5), rep(1,5))
m <- length(w.true)
# create covariance matrix - highly correlated variables.
sigma <- diag(m)
sigma[upper.tri(sigma)] <- 0.75
sigma[lower.tri(sigma)] <- 0.75
# simulate data using above covariance and mean zero - n observations
n <- 100
set.seed(12345)
PHI <- rmvnorm(n, sigma=sigma)
PHI <- cbind(1, PHI) #add in intercept
# simulate outomes - standard normal error added to each one
y <- PHI[,-1] %*% matrix(w.true, ncol=1) + rnorm(n)
e = y - mean(y)
### Gibbs Sampling
# number of iterations
L <- 1000
# Prior on model space using Beta-Binomial. Compute and save it outside the loop for every possible model size.
d.seq <- seq(0, m, 1)
model.prior <- rep(0, m+1)
for (i in 1:(m+1)) {
model.prior[i] <- 1/(m+1) * 1/( factorial(m) / (factorial(m - d.seq[i]) * factorial(d.seq[i])) )
}
# initial gamma
gamma.0 <- rep(0,50)
gamma <- gamma.0
# prior dispersion - we could extend this to run for various values of g
g <- n
#setup matrices to store results in each iteration
gamma.history <- matrix(NA, L, m)
prob.history <- rep(NA,L)
w.history <- matrix(NA, L, m)
# update loop
set.seed(14234)
for (i in 1:L) {
for (j in 1:m) {
#Compute probabilities without var j
#number of parameters and gamma without j
d.0 <- sum(gamma[-j]!=0)
gamma.test.0 <- gamma
gamma.test.0[j] <- 0
#Compute MLE r-squared without j
PHI.0 <- PHI[,c(TRUE,(gamma.test.0==1))]
H.mle.0 <- PHI.0%*%solve(t(PHI.0)%*%PHI.0)%*%t(PHI.0)
e.0 <- (diag(n)-H.mle.0) %*% y
R.sq.0 <- as.numeric( 1 - (t(e.0) %*% e.0) / sum((y - mean(y))^2) )
#Compute second term of marginal likelihood (first term same for all models).
marg.lik.0 <- as.numeric( (1+g)^((n-d.0-2)/2) / (1+g*(1-R.sq.0))^((n-1)/2))
#Compute model probability with var j
#number of parameters and gamma with j
d.1 <- d.0 + 1
gamma.test.1 <- gamma
gamma.test.1[j] <- 1
#Compute MLE r-squared with j
PHI.1 <- PHI[,c(TRUE,(gamma.test.1==1))]
H.mle.1 <- PHI.1%*%solve(t(PHI.1)%*%PHI.1)%*%t(PHI.1)
e.1 <- (diag(n)-H.mle.1) %*% y
R.sq.1 <- as.numeric( 1 - (t(e.1) %*% e.1) / sum((y - mean(y))^2) )
#Compute second term of marginal likelihood with j
marg.lik.1 <- as.numeric( (1+g)^((n-(d.1)-2)/2) / (1+g*(1-R.sq.1))^((n-1)/2) )
# compute probability of new variable being in model
prob <- marg.lik.1 * model.prior[d.1+1] / (marg.lik.1 * model.prior[d.1+1] + marg.lik.0 * model.prior[d.0+1])
# update gamma and variables
if (runif(1) < prob) {
gamma[j] <- 1
}
else {
gamma[j] <- 0
}
}
#Compute results for the new model
d.new <- sum(gamma!=0)
PHI.new <- PHI[,c(TRUE,(gamma==1))]
H.mle.new <- PHI.new%*%solve(t(PHI.new)%*%PHI.new)%*%t(PHI.new)
e.new <- (diag(n)-H.mle.new) %*% y
R.sq.new <- as.numeric( 1 - (t(e.new) %*% e.new) / sum((y - mean(y))^2) )
marg.lik.new <- as.numeric(factorial((n-1)/2 - 1) / (pi^((n-1)/2) * sqrt(n)) * (sqrt(t(e) %*% e))^(1-n) * (1+g)^((n-d.new-2)/2) / (1+g*(1-R.sq.new))^((n-1)/2) )
#Save w estimates (w computed based on the w_bayes estimate expression for Zellner's g prior)
w.history[i,which(gamma!=0)] = ((g/ (g+1)) * solve(t(PHI.new)%*% PHI.new)%*%t(PHI.new)%*%y)[-1]
#Save probabilities
prob.new <- marg.lik.new * model.prior[d.new+1]
prob.history[i] <- marg.lik.new
#Save gamma
gamma.history[i,] <- gamma
}
no.vars <- rowSums(gamma.history)
plot(1:L, no.vars, type="l",xlab="Iteration", ylab="Number of variables in the model", main="Model size")
abline(mean(no.vars), 0, col="red")
library(plyr)
model.prob = data.frame( model = apply( gamma.history,1, function(x) {paste(c(1:50)[as.logical(x)], collapse = ", ")}  ))
model.prob = ddply(model.prob, .(model), nrow)
model.prob$prob = model.prob$V1/L
model.prob = model.prob[order(model.prob$V1, decreasing =T),]
model.prob[1:5,c(1,3)]
cummean <- matrix(NA, L, m)
for (i in 1:L) {
cummean[i,] <- colSums(gamma.history[1:i,,drop=F])/i
}
matplot(1:L, cummean, type="l", lty=1, xlab="Step", ylab="marginal inclusion probability", main="Marginal Inclusion Probabilities for each of the 50 Features", las=1)
plot(cummean[1000,1:40], xlim=c(0,50), ylim=c(0,1), xlab="Predictor", ylab="Posterior probability of non-zero coefficient", main="Marginal Inclusion Probabilities")
points(41:45, cummean[1000, 41:45], col="blue")
points(46:50, cummean[1000, 46:50], col="red")
abline(1, 0, col="red")
abline(0.5, 0, col="blue")
w.history[is.na(w.history)] <- 0
plot(colMeans(w.history, na.rm = TRUE)[1:40], xlim=c(0,50), ylim=c(-0.5, 1.5), xlab="Predictor", ylab="Expected posterior coefficients", main="Bayesian Model Averaging - Estimates of Coefficients")
points(41:45, colMeans(w.history, na.rm = TRUE)[41:45], col="blue")
points(46:50, colMeans(w.history, na.rm = TRUE)[46:50], col="red")
abline(1, 0, col="red")
abline(0.5, 0, col="blue")
library(shiny)
install.packages("shiny")
library(shiny)
library(mtvnorm)
library(mvtnorm)
x <- rmvnorm(100, mean=c(1,5), sigma=1*diag(2))
plot(x[,1], x[,2])
rho <- 0.8
sdx1 <- 2
sdx2 <- 2
covTerm <- rho * sdx1 * sdx2
matrix(c(sdx1^2,covTerm, covTerm, sdx2^2), ncol=2)
x <- rmvnorm(100, mean=c(1,5), sigma=cvc)
plot(x[,1], x[,2])
cvc <- matrix(c(sdx1^2,covTerm, covTerm, sdx2^2), ncol=2)
x <- rmvnorm(100, mean=c(1,5), sigma=cvc)
plot(x[,1], x[,2])
setwd("~/Desktop/BGSE/AdvancedCompMethods/Project/predicting-online-news-popularity/data")
library(h2o)
h2o.init(nthreads = -1)
path1 <- "training70.csv"
path2 <- "test30.csv"
training.hex <- h2o.uploadFile(path = path1, destination_frame = "training.hex" )
test.hex <- h2o.uploadFile(path = path2, destination_frame = "test.hex" )
training.gbm <- h2o.gbm(y=63, x=5:62, training_frame= training.hex, ntrees=20, max_depth=5, min_rows= 2, learn_rate=0.01, distribution="multinomial")
training.hex$popularity <- as.factor(training.hex$popularity)
?h2o.gbm
training.gbm <- h2o.gbm(y=63, x=5:62, training_frame= training.hex, ntrees=100,
max_depth=20, min_rows= 20, learn_rate=0.01,
distribution="multinomial", nbins=5)
training.gbm@model$training_metrics
prediction <- h2o.predict(training.gbm, newdata=test.hex)
pred <- as.data.frame(prediction)
actual <- as.data.frame(test.hex)[,63]
percent_correct <- sum(pred[,1] == actual) / length(actual)
percent_correct
training.gbm <- h2o.gbm(y=63, x=5:62, training_frame= training.hex, ntrees=100,
max_depth=20, min_rows= 20, learn_rate=0.005,
distribution="multinomial", nbins=5)
training.gbm@model$training_metrics
prediction <- h2o.predict(training.gbm, newdata=test.hex)
pred <- as.data.frame(prediction)
head(pred)
actual <- as.data.frame(test.hex)[,63]
percent_correct <- sum(pred[,1] == actual) / length(actual)
percent_correct
?h2o.predict
aaa <- test.hex[,5:62]
head(aaa)
class(aaa)
prediction <- h2o.predict(training.gbm, newdata=test.hex[,5:62])
pred <- as.data.frame(prediction)
actual <- as.data.frame(test.hex)[,63]
percent_correct <- sum(pred[,1] == actual) / length(actual)
percent_correct
training.gbm <- h2o.gbm(y=63, x=5:62, training_frame= training.hex, ntrees=100,
max_depth=20, min_rows= 20, learn_rate=0.05,
distribution="multinomial", nbins=5)
training.gbm@model$training_metrics
prediction <- h2o.predict(training.gbm, newdata=test.hex[,5:62])
pred <- as.data.frame(prediction)
actual <- as.data.frame(test.hex)[,63]
percent_correct <- sum(pred[,1] == actual) / length(actual)
percent_correct
ntrees_opt <- c(5,25,50,100)
maxdepth_opt <- c(2,5,15,35)
learnrate_opt <- c(0.001, 0.05, 0.1, 0.2)
hyper_parameters <- list(ntrees=ntrees_opt, max_depth=maxdepth_opt, learn_rate=learnrate_opt)
?h2o.grid
grid <- h2o.grid("gbm", hyper_params = hyper_parameters, y = 63, x = 5:62,
distribution="multinomial", training_frame= training.hex,
validation_frame=test.hex )
grid
grid_models <- lapply(grid@model_ids, function(model_id) {model = h2o.getModel(model_id)})
for (i in 1:length(grid_models)) {
print(sprintf("auc: %f", h2o.auc(grid_models[[i]])))
}
print(sprintf("auc: %f", h2o.auc(grid_models[[1]])))
h2o.auc(grid_models[[1]])
class(grid_models)
str(grid_models)
grid
?lapply
?h2o.getModel
dim(grid_models)
dim(grid)
grid_models[[1]]
h2o.auc(grid_models[[1]])
?h2o.auc
class(grid_models[[1]])
h2o.confusionMatrix(grid_models[[1]])
?h2o.confusionMatrix
h2o.confusionMatrix(grid_models[[1]], metrics = "accuracy")
h2o.confusionMatrix(grid_models[[1]], metrics = accuracy)
h2o.confusionMatrix(grid_models[[1]], metrics = precision)
confMatrix <- h2o.confusionMatrix(grid_models[[1]])
View(confMatrix)
View(confMatrix)
precisions <- vector()
for (i in 1:length(grid_models)) {
precisions <- c(vector, h2o.confusionMatrix(grid_models[[i]])[6,6])
}
precisions <- rep(0, length(grid_models))
for (i in 1:length(grid_models)) {
precisions[i] <- h2o.confusionMatrix(grid_models[[i]])[6,6]
}
plot(precision)
plot(precisions)
max(precisions)
training.gbm <- h2o.gbm(y=63, x=5:62, training_frame= training.hex, ntrees=2000,
max_depth=5, min_rows= 20, learn_rate=0.05,
distribution="multinomial", nbins=50)
training.gbm@model$training_metrics
prediction <- h2o.predict(training.gbm, newdata=test.hex)
pred <- as.data.frame(prediction)
head(pred)
actual <- as.data.frame(test.hex)[,63]
percent_correct <- sum(pred[,1] == actual) / length(actual)
percent_correct
