abline(0.5, 0, col="blue")
library(shiny)
install.packages("shiny")
library(shiny)
library(mtvnorm)
library(mvtnorm)
x <- rmvnorm(100, mean=c(1,5), sigma=1*diag(2))
plot(x[,1], x[,2])
rho <- 0.8
sdx1 <- 2
sdx2 <- 2
covTerm <- rho * sdx1 * sdx2
matrix(c(sdx1^2,covTerm, covTerm, sdx2^2), ncol=2)
x <- rmvnorm(100, mean=c(1,5), sigma=cvc)
plot(x[,1], x[,2])
cvc <- matrix(c(sdx1^2,covTerm, covTerm, sdx2^2), ncol=2)
x <- rmvnorm(100, mean=c(1,5), sigma=cvc)
plot(x[,1], x[,2])
library(devtools)
install_github( "aspeijers/predicting-online-news-popularity/hamclass")
install_github( "aspeijers/predicting-online-news-popularity/hamclass", auth_token = "e5fc4b5f96694b436e1f183884722398d98f9764" )
?hamclass
load("~/Desktop/BGSE/AdvancedCompMethods/Project/predicting-online-news-popularity/Code/randomForest.R")
?points
pi
?lines
?points
?axis
?points
if (!require("knitr")) install.packages("knitr"); library(knitr)
if (!require("rmarkdown")) install.packages("rmarkdown"); library(rmarkdown)
opts_chunk$set(comment='##', warning=FALSE, message=FALSE, include=TRUE, echo=TRUE, cache=TRUE, cache.comments=FALSE)
output <- opts_knit$get("rmarkdown.pandoc.to")
if (output=="html") opts_chunk$set(fig.width=10, fig.height=5)
if (output=="latex") opts_chunk$set(fig.width=6,  fig.height=4, dev = 'cairo_pdf', dev.args=list(family="Arial"))
genEasy <- function(noObs = 100) {
noFeatures <- 10
X <- matrix(runif(noObs*noFeatures), ncol = noFeatures)
y <- ifelse(X[,1] > 0.5, 1, 0)
return(list(X=X, y=y))
}
genDifficult <- function(noObs = 100) {
noFeatures <- 10
X <- matrix(runif(noObs*noFeatures), ncol = noFeatures)
y <- ifelse(sign((X[,1]-0.5)*(X[,2]-0.5)*(X[,3]-0.5)) > 0, 1, 0)
return(list(X=X, y=y))
}
if (!require("doMC")) install.packages("doMC")
if (!require("class")) install.packages("class")
if (!require("dplyr")) install.packages("dplyr")
if (!require("ggplot2")) install.packages("ggplot2")
trainEasy <- genEasy(100)
testEasy <- genEasy(1000)
trainDiff <- genDifficult(100)
testDiff <- genDifficult(1000)
ks <- c(1:60)
getResubError <- function(features, labels, k) {
pred <- knn(train = features, test = features, cl = labels, k = k)
return(c(k = k, error = mean(pred != labels)))
}
registerDoMC(detectCores())
install.packages("doMC")
library(doMC)
registerDoMC(detectCores())
resubError <- foreach(k = ks,
.combine = rbind,
.packages = c("class", "dplyr")) %dopar% {
# some helpful debugging messages
#cat("The current test set! k=", k, "\n")
# kNN results
resEasy <- getResubError(trainEasy$X, trainEasy$y, k)
resDiff <- getResubError(trainDiff$X, trainDiff$y, k)
# final output
data.frame(c("Resub", "Resub"), c("Easy", "Difficult"),
rbind(resEasy, resDiff))
}
colnames(resubError) <- c("cvType", "exp", "k", "Error")
resubError <- cbind(resubError, SE=NA)
genCVid <- function(noObs = 100, K = 10, seed = round(runif(1)*10000, 0)) {
stopifnot(noObs >= K); set.seed(seed)
CVindex <- rep(1:K, times = ceiling(noObs / K))[1:noObs]
CVindexRand <- sample(CVindex, noObs)
}
getKfoldError <- function(features, labels, k, bucket) {
pred <- knn(train = features[cvID!=bucket,],
test = features[cvID==bucket,],
cl = labels[cvID!=bucket],
k = k)
error <- mean(pred != labels[cvID==bucket])
return(c(k = k, bucket = bucket, error = error))
}
noBuckets <- nrow(trainEasy$X)
cvID <- genCVid(noBuckets, noBuckets, 1111)
looError <- foreach(bucket = rep(1:noBuckets, length(ks)),
k = rep(ks, each=noBuckets),
.combine = rbind,
.packages = c("class", "dplyr")) %dopar% {
# some helpful debugging messages
#cat("Bucket", bucket, "is the current test set! k=", k, "\n")
# kNN results
resEasy <- getKfoldError(trainEasy$X, trainEasy$y, k, bucket)
resDiff <- getKfoldError(trainDiff$X, trainDiff$y, k, bucket)
# final output
data.frame(c("Loo", "Loo"), c("Easy", "Difficult"),
rbind(resEasy, resDiff))
}
colnames(looError) <- c("cvType", "exp", "k", "bucket", "error")
looError <- as.data.frame(looError) %>%
group_by(cvType, exp, k) %>%
summarize(Error = mean(error),
SE = sd(error)/mean(n()))
noBuckets <- 10
cvID <- genCVid(nrow(trainEasy$X), noBuckets, 1111)
kfoldError <- foreach(bucket = rep(1:noBuckets, length(ks)),
k = rep(ks, each=noBuckets),
.combine = rbind,
.packages = c("class", "dplyr")) %dopar% {
# some helpful debugging messages
#cat("Bucket", bucket, "is the current test set! k=", k, "\n")
# kNN results
resEasy <- getKfoldError(trainEasy$X, trainEasy$y, k, bucket)
resDiff <- getKfoldError(trainDiff$X, trainDiff$y, k, bucket)
# final output
data.frame(c("10fold", "10fold"), c("Easy", "Difficult"),
rbind(resEasy, resDiff))
}
colnames(kfoldError) <- c("cvType", "exp", "k", "bucket", "error")
kfoldError <- kfoldError %>%
group_by(cvType, exp, k) %>%
summarize(Error = mean(error),
SE = sd(error)/mean(n()))
noMC <- nrow(trainEasy$X)
noBuckets <- noMC/10
mcError <- foreach(bucket = rep(1:noBuckets, length(ks)*noMC/10),
k = rep(ks, each=noMC),
.combine = rbind,
.packages = c("class", "dplyr")) %dopar% {
# some helpful debugging messages
#cat("Bucket", bucket, "is the current test set! k=", k, "\n")
# randomly subsetting in each iteration
cvID <- genCVid(nrow(trainEasy$X), noBuckets)
# kNN results
resEasy <- getKfoldError(trainEasy$X, trainEasy$y, k, bucket)
resDiff <- getKfoldError(trainDiff$X, trainDiff$y, k, bucket)
# final output
data.frame(c("MC", "MC"), c("Easy", "Difficult"),
rbind(resEasy, resDiff))
}
colnames(mcError) <- c("cvType", "exp", "k", "bucket", "error")
mcError <- mcError %>%
group_by(cvType, exp, k) %>%
summarize(Error = mean(error),
SE = sd(error)/mean(n()))
getBIC <- function(features, labels, k) {
pred <- knn(train = features, test = features, cl = labels, k = k,
prob = TRUE)
noObs <- length(labels)
bic <- -2*sum(log(attr(pred, "prob"))) + (noObs/k)*log(noObs)
return(c(k = k, error = bic))
}
bicScore <- foreach(k = ks,
.combine = rbind,
.packages = c("class", "dplyr")) %dopar% {
# some helpful debugging messages
#cat("The current test set! k=", k, "\n")
# kNN results
resEasy <- getBIC(trainEasy$X, trainEasy$y, k)
resDiff <- getBIC(trainDiff$X, trainDiff$y, k)
# final output
data.frame(c("BIC", "BIC"), c("Easy", "Difficult"),
rbind(resEasy, resDiff))
}
colnames(bicScore) <- c("cvType", "exp", "k", "Error")
bicScore <- cbind(bicScore, SE=NA)
# so that we can illustrate it together with CV errors on the same figure
er <- bicScore$Error
bicScore$Error <- (er-min(er))/(max(er)- min(er))
getTestError <- function(features, labels, k, testFeatures, testLabels) {
pred <- knn(train = features, test = testFeatures, cl = labels, k = k)
return(c(k = k, error = mean(pred != testLabels)))
}
testError <- foreach(k = ks,
.combine = rbind,
.packages = c("class", "dplyr")) %dopar% {
# some helpful debugging messages
#cat("The current test set! k=", k, "\n")
# kNN results
resEasy <- getTestError(trainEasy$X, trainEasy$y, k,
testEasy$X, testEasy$y)
resDiff <- getTestError(trainDiff$X, trainDiff$y, k,
testDiff$X, testDiff$y)
# final output
data.frame(c("Test", "Test"), c("Easy", "Difficult"),
rbind(resEasy, resDiff))
}
registerDoSEQ()
colnames(testError) <- c("cvType", "exp", "k", "Error")
testError <- cbind(testError, SE=NA)
results <- rbind(resubError, looError, kfoldError,
mcError, bicScore, testError)
ggplot(data = results,
aes(x = factor(k), y = Error, colour=cvType, group=cvType)) +
geom_line() +
geom_point() +
facet_wrap(~exp, ncol=1) +
scale_x_discrete("k -Number of nearest neighbors",
breaks = c(1, seq(5,60,5))) +
ylab("Misclassification error") +
theme_bw(base_size = 14, base_family = "Helvetica")
findThreshold <- function(x, y) {
noPoints <- length(x)
errors <- rep(NA, noPoints-1)
thresholds <- rep(NA, noPoints-1)
splitLabels <- matrix(NA, ncol=2, nrow=noPoints-1)
# we go sequentially over each point and cut between that point and the
# closest neighbor
for (idx in 1:(noPoints-1)) {
# locate a potential threshold, a split between two points
potThres <- mean(x[idx:(idx+1)])
# check the classification error, when both sides,
# are classified with mean label
predictedClasses <- rep(NA, noPoints)
meanLeft <- mean(y[x < potThres])
meanRight <- mean(y[x >= potThres])
if (meanLeft < 0.5) {
predictedClasses[x < potThres] <- 0
} else {
predictedClasses[x < potThres] <- 1
}
if (meanRight < 0.5) {
predictedClasses[x > potThres] <- 0
} else {
predictedClasses[x > potThres] <- 1
}
# error of this split
misError <- mean(predictedClasses != y)
# recording the accuracy, thresholds and labels of
# the splitted interval
errors[idx] <- misError
thresholds[idx] <- potThres
splitLabels[idx,] <- c(predictedClasses[x < potThres][1],
predictedClasses[x > potThres][1])
}
# print(cbind(errors, thresholds, splitLabels))
# next we find the minimum and the best threshold
minError <- min(errors)
bestThreshold <- thresholds[which(errors==minError)]
# if more than 1 threshold has the same accuracy we choose one randomly
bestThreshold <- sample(bestThreshold, 1)
# what are the final labels of the best split?
labels <- splitLabels[which(thresholds==bestThreshold),]
# print(cbind(minError, bestThreshold, labels))
return(list(thres = bestThreshold,
err = minError,
labels = labels))
}
?cut
aaa <- seq(0,10,.1)
aaa <- seq(0, 10, 1)
aaa
cut(aaa, c(0,1), include.lowest = TRUE)
Z <- stats::rnorm(10000)
table(cut(Z, breaks = -6:6))
aaa <- cut(Z, breaks= -6:6)
aaa
Z <- stats::rnorm(1000)
aaa <- cut(Z, breaks= -6:6)
aaa
table(cut(Z, breaks = -6:6))
table(cut(Z, breaks = -6:6, include.lowest = ))
table(cut(Z, c(0,1), include.lowest = ))
Z
?append
head(trainEasy)
head(trainDiff)
load("~/Desktop/BGSE/AdvancedCompMethods/Project/predicting-online-news-popularity/Code/randomForest_workspace.RDataTmp")
load("~/Desktop/BGSE/AdvancedCompMethods/Project/predicting-online-news-popularity/Code/randomForest_workspace.RDataTmp")
load("~/Desktop/BGSE/AdvancedCompMethods/Project/predicting-online-news-popularity/Code/randomForest_workspace.RData")
load("~/Desktop/BGSE/AdvancedCompMethods/Problemset Info/PS6_workspace.RData")
load("~/Desktop/BGSE/Financial Econs/PS5/PS5_workspace.RData")
library(randomForest)
library(forecast)
library(gbm)
forecast.data <- read.csv("~/Documents/Semester_2/Financial Econometrics/forecast-competition-data.csv")
onestp <- function(y){
require(forecast)
coln = c(1,4,5,11,13,16,21,24,27,28,35,38,40,41,46)
nrow = nrow(y)
y = y[,coln]
t = y[-1,1]
xreg = scale(y[,-1])
n = 3
order = matrix(c(0:(n-1),rep(0,n),rep(0,n)),nrow= n)
model <- apply(order,1,function(x) arima(t,xreg = xreg[-nrow,],order = x))
newxreg = matrix(xreg[nrow,],ncol=ncol(xreg))
onestep.for <- vapply(model,function(x) as.numeric(predict(x,
newxreg = newxreg,n.ahead = 1)$pred),FUN.VALUE = 0)
print(unlist(onestep.for))
pred <- mean(unlist(onestep.for))
}
rf.var <- function(y){
nrow = nrow(y)
x = matrix(NA,nrow= nrow - 4, ncol= 250)
for(i in 1:(nrow-4)){
x[i,] <- as.vector(unlist(y[i:(i+4),]))
}
t = na.omit(shift(y$TARGET,5,type="lead"))
nrowx = nrow(x)
row.names(x) <- 1:nrowx
rf <- randomForest(x =x[-nrowx,],y = t,xtest= matrix(x[nrowx,],nrow = 1),
importance=TRUE)
coln <- unique(c(1,ceiling(as.numeric(names(sort(rf$importance[,2],decreasing = TRUE)))/5)[1:15]))
print(coln)
pred <- rf$test$predicted
}
rf.model <- function(y){
coln = c(1,4,5,11,13,16,21,24,27,28,35,38,40,41,46)
y = y[,coln]
nrow = nrow(y)
lag = 1
x = matrix(NA,nrow= nrow - lag, ncol= 15*(lag+1))
for(i in 1:(nrow-lag)){
x[i,] <- as.vector(unlist(y[i:(i+lag),]))
}
t = na.omit(shift(y$TARGET,lag+1,type="lead"))
nrowx = nrow(x)
row.names(x) <- 1:nrowx
rf <- randomForest(x =x[-nrowx,],y = t, xtest= matrix(x[nrowx,],nrow = 1), ntree = 50, nodesize = 1)
pred <- rf$test$predicted
}
gbm.model <- function(y){
coln = c(1,4,5,11,13,16,21,24,27,28,35,38,40,41,46)
y = y[,coln]
nrow = nrow(y)
lag = 1
x = matrix(NA,nrow= nrow - lag, ncol= 15*(lag+1))
for(i in 1:(nrow-lag)){
x[i,] <- as.vector(unlist(y[i:(i+lag),]))
}
t = na.omit(shift(y$TARGET,lag+1,type="lead"))
nrowx = nrow(x)
row.names(x) <- 1:nrowx
xtest= matrix(x[nrowx,],nrow = 1)
fit <- gbm.fit(x=x[-nrowx,], y = t, distribution = "gaussian")
f.predict <- predict(fit,xtest,100)
}
onestp <- function(y){
require(randomForest)
coln = c(1,4,5,11,13,16,21,24,27,28,35,38,40,41,46)
y = y[,coln]
nrow = nrow(y)
lag = 1
x = matrix(NA,nrow= nrow - lag, ncol= 15*(lag+1))
for(i in 1:(nrow-lag)){
x[i,] <- as.vector(unlist(y[i:(i+lag),]))
}
t = na.omit(shift(y$TARGET,lag+1,type="lead"))
nrowx = nrow(x)
row.names(x) <- 1:nrowx
rf <- randomForest(x =x[-nrowx,],y = t, xtest= matrix(x[nrowx,],nrow = 1), ntree = 50, nodesize = 1)
pred <- rf$test$predicted
}
install.packages("data.table")
?shift
remove.package(data.table)
remove.packages(data.table)
remove.packages("data.table")
install.packages("data.table", repos = "https://Rdatatable.github.io/data.table", type = "source")
library(data.table)
setwd("~/Desktop/BGSE/AdvancedCompMethods/Project/predicting-online-news-popularity/code")
data <- read.csv("../data/training70.csv", header = TRUE)
X <- data[,4:62]
popularity <- data[,63]
combined <- cbind(X, popularity)
combined$popularity <- as.factor(combined$popularity)
features <- names(X)
data <- read.csv("../data/test30.csv", header = TRUE)
X_test <- data[,4:62]
combined <- combined[combined$n_unique_tokens<=1, ]
combined <- combined[combined$n_non_stop_words<=1, ]
combined <- combined[combined$n_non_stop_unique_tokens<=1, ]
varType <- rep( NA, length(X) )
for ( i in 1:length(X) ) {
type <- class( X[,i] )
if (type=="int") {
if (length(unique(X[,i])) > 2) {
varType[i] <- "count"
} else {
varType[i] <- "binary"
}
} else {
varType[i] <- type
}
}
combinedCont <- cbind( combined[,(varType=="numeric")], popularity=as.factor(combined$popularity))
X_testCont <- X_test[ ,(varType=="numeric")]
featuresCont <- names(combinedCont)
varDiff1 <- vector() # vectors of interaction terms to include (term1 in varDiff1 and term2 in varDiff2)
varDiff2 <- vector()
combined_updated1 <- combined
X_test_updated1 <- X_test
combined_updated2 <- combined
X_test_updated2 <- X_test
for ( i in 1:(length(combinedCont)-1) ) {
for ( j in i:(length(combinedCont)-1) ) {
interact <- combinedCont[,i]*combinedCont[,j]
XX.anov <- aov( interact ~ combinedCont$popularity )
if (sum( TukeyHSD(XX.anov)[[1]][,4] < 0.05 ) == 10) {     # if null is NOT accepted if (p-value < 0.05) (ie means are not equal at significance level 5%),
varDiff1 <- c(varDiff1, featuresCont[i])
varDiff2 <- c(varDiff2, featuresCont[j])                # interaction term may help us discriminate between categories.
combined_updated1 <- data.frame( cbind( interact, combined_updated1 ) )
interact <- X_testCont[,i]*X_testCont[,j]
X_test_updated1 <- data.frame( cbind( interact, X_test_updated1 ) )
}
}
}
combined_updated2 <- combined_updated1[, -(which(  (names(combined_updated1) %in% varDiff1) | (names(combined_updated1) %in% varDiff2)  ))]
X_test_updated2 <- X_test_updated1[, -(which(  (names(X_test_updated1) %in% varDiff1) | (names(X_test_updated1) %in% varDiff2)  ))]
names(combined_updated2)
varDiff1
varDiff2
interactWanted <- c("interact", "interact.3", "interact.4", "interact.5", "interact.7", "interact.8",)
logicalVec <- names(combined_updated2 %in% interactWanted)
interactWanted <- c("interact", "interact.3", "interact.4", "interact.5",
"interact.7", "interact.8", "interact.11", "interact.12",
"interact.16", "interact.17", "interact.18", "interact.20",
"interact.26", "interact.27", "interact.29")
logicalVec <- names(combined_updated2 %in% interactWanted)
aaa <- names(combined_updated2)
logicalVec <- (names(combined_updated2) %in% interactWanted)
length(names(combined_updated2))
length(interactWanted)
logicalVec <- (names(combined_updated2)[1:30] %in% interactWanted)
table(logicalVec)
var1 <- varDiff1[logicalVec]
var2 <- varDiff2[logicalVec]
setwd("~/Desktop/BGSE/AdvancedCompMethods/Project/predicting-online-news-popularity/Data")
train       <- read.csv("../data/news_popularity_training.csv", header = TRUE)
train       <- train[,3:62]
X_train     <- train[,1:59]
Y_train     <- factor(train[,60])
test        <- read.csv("../data/news_popularity_test.csv", header = TRUE)
test        <- test[,3:61]
X_test      <- test
varType     <- rep( NA, length(X_train) )
for ( i in 1:length(X_train) ) {
varType[i] <- class( X_train[,i] )
}
X_train_cont    <- X_train[,(varType=="numeric")]
X_test_cont     <- X_test[ ,(varType=="numeric")]
features_cont   <- names(train_cont)
p_cont  <- ncol(X_test_cont)
term1 <- vector()
term2 <- vector()
interactions_train <- vector()
interactions_test  <- vector()
for ( i in 1:p_cont ) {
if (i<p_cont) {
for ( j in (i+1):p_cont ) {
# construct potential interaction term
interact_train <- X_train_cont[,i]*X_train_cont[,j]
# run ANOVA for potential interaction term regressed on the labels
XX_anov <- aov( interact_train ~ Y_train )
# calculate TukeyHSD to determine which means are significantly different
# if all (10) means are statistically significantly different at 5% level, accept the interaction variable
if (sum( TukeyHSD(XX_anov)[[1]][,4] < 0.05 ) == 10) {     # if null is NOT accepted if (p-value < 0.05) (ie means are not equal at significance level 5%),
term1 <- c(term1, features_cont[i])
term2 <- c(term2, features_cont[j])
# add interaction term to data frame - training set
interactions_train <- data.frame(cbind(interact_train, interactions_train))
# add same interaction term for test set
interact_test <- X_test_cont[,i]*X_test_cont[,j]
interactions_test <- data.frame(cbind(interact_test, interactions_test))
}
}
}
}
features_cont   <- names(train_cont)
features_cont   <- names(X_train_cont)
p_cont  <- ncol(X_test_cont)
term1 <- vector()
term2 <- vector()
interactions_train <- vector()
interactions_test  <- vector()
for ( i in 1:p_cont ) {
if (i<p_cont) {
for ( j in (i+1):p_cont ) {
# construct potential interaction term
interact_train <- X_train_cont[,i]*X_train_cont[,j]
# run ANOVA for potential interaction term regressed on the labels
XX_anov <- aov( interact_train ~ Y_train )
# calculate TukeyHSD to determine which means are significantly different
# if all (10) means are statistically significantly different at 5% level, accept the interaction variable
if (sum( TukeyHSD(XX_anov)[[1]][,4] < 0.05 ) == 10) {     # if null is NOT accepted if (p-value < 0.05) (ie means are not equal at significance level 5%),
term1 <- c(term1, features_cont[i])
term2 <- c(term2, features_cont[j])
# add interaction term to data frame - training set
interactions_train <- data.frame(cbind(interact_train, interactions_train))
# add same interaction term for test set
interact_test <- X_test_cont[,i]*X_test_cont[,j]
interactions_test <- data.frame(cbind(interact_test, interactions_test))
}
}
}
}
var1
var2
term1
term2
names(interactions_train)
cbind(var1, var2)
cbind(term1, term2)
