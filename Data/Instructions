The instructions: 

So I am uploading 3 files total:
1) train_result.csv
2) test_result.csv
3) labels.csv

The train_result.csv files has 21000 rows and 36 columns
It is a modification of our dataset training70.csv

The test_result.csv file has 9000 rows and 35 columns 
It has the same modifications done to training70 but it is the modified test30.csv

Labels.csv consists of the labels from the test30 set, I separated the test_result.csv and labels.csv 
for my convenience. 

The code H20-new.R has the part till the percentage calculation modified such that you can
just execute it and it will run smoothly, only the fine tuning of the parameters needs to be 
done. 

The models I have executed are as follows:
The values inside the brackets are respectively (ntrees, max_depth, min_rows, learn_rate) 
The accuracy is the value listed outside. (not correct cause of the selection of the x on the training matrix)
(150,3,10,0.01) -> 0.4933
(1000,7,5,0.001) -> 0.51277
(100,7,5,0.01) -> 0.51277
(400,6,10,0.02) -> 0.51833 
(500, 4, 10, 0.01) -> 0.52
(200, 5, 10, 0.01) -> 0.52088
(400, 5, 10, 0.01) -> 0.52255
(300, 5, 10, 0.01) -> 0.5235
(250, 5, 10, 0.01) -> 0.5242 


After we selected the x from 2:35

(500, 5, 10, 0.01) -> 0.5221
(230, 5, 10, 0.01) -> 0.5207
(240, 5, 10, 0.01) -> 0.5219
(250, 5, 10, 0.01) -> 0.523
(265, 5, 10, 0.01) -> 0.5226
(280, 5, 10, 0.01) -> 0.5231***
(300, 5, 10, 0.01) -> 0.5225

(280, 15, 10, 0.01) -> 0.5057
(280, 3, 10, 0.01) -> 0.5163
(280, 4, 10, 0.01) -> 0.5182
(280, 7, 10, 0.01) -> 0.5239
(280, 6, 10, 0.01) -> 0.5252**

(280, 6, 5, 0.01) -> 0.5213
(280, 6, 8, 0.01) -> 0.5213
(280, 6, 12, 0.01) -> 0.5236
(280, 6, 20, 0.01) -> 0.5244**

(280, 6, 10, 0.001) -> 0.5053
(280, 6, 10, 0.005) -> 0.5181
(280, 6, 10, 0.05) -> 0.5117
(280, 6, 10, 0.0001) -> 0.4969
(280, 6, 10, 0.02) -> 0.5206**

(257, 6, 20, 0.01) -> 0.525
(285, 6, 20, 0.01) -> 0.5257***
(290, 6, 20, 0.01) -> 0.5257
(295, 6, 20, 0.01) -> 0.5253

Results of xgboost49 parameter tunning

ntrees, maxdepth, minrow, learn_rate and nfold=5

(500, 5, 10, 0.01) -> 0.5276
(230, 5, 10, 0.01) -> 0.5242
(600, 5, 10, 0.01) -> 0.528
(700, 5, 10, 0.01) -> 0.5268
(650, 5, 10, 0.01) -> 0.5281
(550, 5, 10, 0.01) -> 0.5276
(625, 5, 10, 0.01) -> 0.5282**
(675, 5, 10, 0.01) -> 0.5277

(1500, 8, 0.005) -> 0.5223

(625, 4, 10, 0.01) -> 0.5232
(625, 6, 10, 0.01) -> 0.5286**
(625, 7, 10, 0.01) -> 0.5268
(625, 8, 10, 0.01) -> 0.5201

(625, 6, 5, 0.01) -> 0.5264
(625, 6, 8, 0.01) -> 0.5274
(625, 6, 12, 0.01) -> 0.5284**
(625, 6, 11, 0.01) -> 0.5276

(625, 6, 10, 0.001) -> 0.5178
(625, 6, 10, 0.005) -> 0.5263**
(625, 6, 10, 0.05) -> 0.5168
(625, 6, 10, 0.0001) -> 0.5159
(625, 6, 10, 0.02) -> 0.5263**

(625, 6, 13, 0.015) -> 0.5273
(625, 6, 13, 0.01) -> 0.5287
(625, 6, 14, 0.01) -> 0.5302**
(625, 6, 15, 0.01) -> 0.5297

(630, 6, 15, 0.01) -> 0.5292
(640, 6, 15, 0.01) -> 0.5293
(645, 6, 15, 0.01) -> 0.5289
(620, 6, 15, 0.01) -> 0.5296**
(623, 6, 15, 0.01) -> 0.5296

(630, 6, 14, 0.01) -> 0.5301
(640, 6, 14, 0.01) -> 0.5297
(628, 6, 14, 0.01) -> 0.5299
(620, 6, 14, 0.01) -> 0.5297
(626, 6, 14, 0.01) -> 0.5303***

(626, 7, 14, 0.01) -> 0.5284
(626, 6, 14, 0.015) -> 0.5253
(626, 6, 14, 0.007) -> 0.5252


Results of xgboost50 parameter tunning

ntrees, maxdepth, minrow, learn_rate and nfold=5

(500, 5, 10, 0.01) -> 0.5253
(400, 5, 10, 0.01) -> 0.5253
(450, 5, 10, 0.01) -> 0.5246
(350, 5, 10, 0.01) -> 0.5267***
(300, 5, 10, 0.01) -> 0.5252
(375, 5, 10, 0.01) -> 0.5259
(365, 5, 10, 0.01) -> 0.5264
(325, 5, 10, 0.01) -> 0.5251
(550, 5, 10, 0.01) -> 0.524
(600, 5, 10, 0.01) -> 0.5257
(650, 5, 10, 0.01) -> 0.5251

(350, 6, 10, 0.01) -> 0.5244**
(350, 7, 10, 0.01) -> 0.5218
(350, 4, 10, 0.01) -> 0.522
(350, 3, 10, 0.01) -> 0.5198
(350, 9, 10, 0.01) -> 0.5228

(350, 5, 5, 0.01) -> 0.5238
(350, 5, 8, 0.01) -> 0.5259
(350, 5, 12, 0.01) -> 0.5262
(350, 5, 14, 0.01) -> 0.5276**

(350, 5, 14, 0.02) -> 0.5282
(350, 5, 14, 0.025) -> 0.5283***
(350, 5, 14, 0.005) -> 0.5183
(350, 5, 14, 0.001) -> 0.5063